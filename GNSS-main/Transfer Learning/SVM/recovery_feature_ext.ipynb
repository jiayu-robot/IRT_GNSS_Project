{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c2ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Config] 正在使用的设备: cpu ---\n",
      "Key: 0, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\observation_data_case5_with_sat_type.csv\n",
      "Key: 1, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\Case1_Urban_10Hz_with_sat_type.csv\n",
      "Key: 2, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\Case1_Suburban_10Hz_with_sat_type.csv\n",
      "Key: 3, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\Case2_Urban_10Hz_with_sat_type.csv\n",
      "Key: 4, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\Case2_Suburban_10Hz_with_sat_type.csv\n",
      "Key: 5, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\Case3_Urban_10Hz_with_sat_type.csv\n",
      "Key: 6, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\Case3_Suburban_10Hz_with_sat_type.csv\n",
      "Key: 7, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\Case4_Urban_10Hz_with_sat_type.csv\n",
      "Key: 8, Path: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\data_with_sat_type\\Case4_Suburban_10Hz_with_sat_type.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from tqdm import tqdm\n",
    "from utility_uad_svm import load_data, make_sequences, create_dataloaders, SeqDataset\n",
    "\n",
    "#BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "base_dir = os.getcwd()\n",
    "#print(\"当前工作目录:\", base_dir)\n",
    "parent_dir = os.path.dirname(os.path.dirname(base_dir))\n",
    "csv_path = {0: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"observation_data_case5_with_sat_type.csv\"),\n",
    "            1: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"Case1_Urban_10Hz_with_sat_type.csv\"),\n",
    "            2: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"Case1_Suburban_10Hz_with_sat_type.csv\"),\n",
    "            3: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"Case2_Urban_10Hz_with_sat_type.csv\"),\n",
    "            4: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"Case2_Suburban_10Hz_with_sat_type.csv\"),\n",
    "            5: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"Case3_Urban_10Hz_with_sat_type.csv\"),\n",
    "            6: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"Case3_Suburban_10Hz_with_sat_type.csv\"),\n",
    "            7: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"Case4_Urban_10Hz_with_sat_type.csv\"),\n",
    "            8: os.path.join(parent_dir, \"Transfer Learning\\data_with_sat_type\", \"Case4_Suburban_10Hz_with_sat_type.csv\"),\n",
    "            }\n",
    "for key, path in csv_path.items():\n",
    "    print(f\"Key: {key}, Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c6aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SEQ_LEN = 10        # 序列长度 (例如 10 个历元)\n",
    "    STEP = 1            # 滑动窗口步长\n",
    "    BATCH_SIZE = 64     # 批次大小\n",
    "    \n",
    "    # 请根据你的 CSV 实际列名修改这里！\n",
    "    # 假设 CSV 中有这些列:\n",
    "    FEATURE_COLS = [\"cn0\", \"elevation\", \"pseudorange_corrected_cb\", \"doppler_shift\",\"sat_type\"] \n",
    "    TARGET_COL = 'multipath'     # 标签列 (LOS/NLOS)\n",
    "    GROUP_COL = 'sv_id'      # 卫星 ID 列\n",
    "    TIME_COL = 'gps_time'    # 时间戳列\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c70778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] 正在读取 CSV 文件...\n",
      "  -> 已加载真实数据 (Target): 11243 行\n",
      "  -> 已合并仿真数据 (Source): 710063 行\n",
      "\n",
      "[2/4] 正在进行归一化处理...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 118\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m source_loader, target_loader\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# 执行\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# 确保这一步之前已经运行了你的 import os ... csv_path 代码\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m source_loader, target_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m, in \u001b[0;36mget_dataloaders\u001b[1;34m(csv_path_dict)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[2/4] 正在进行归一化处理...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 1. 归一化源域 (Simulation)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m scaler_sim \u001b[38;5;241m=\u001b[39m \u001b[43mMinMaxScaler\u001b[49m()\n\u001b[0;32m     41\u001b[0m sim_df_all[config\u001b[38;5;241m.\u001b[39mFEATURE_COLS] \u001b[38;5;241m=\u001b[39m scaler_sim\u001b[38;5;241m.\u001b[39mfit_transform(sim_df_all[config\u001b[38;5;241m.\u001b[39mFEATURE_COLS])\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 2. 归一化目标域 (Real World)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 单独 fit 目标域，以将巨大的残差映射到 [0, 1] 范围，方便 LSTM 提取相对特征\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "def get_dataloaders(csv_path_dict):\n",
    "    # ---------------------------------------------------------\n",
    "    # A. 读取数据 (Load Data)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[1/4] 正在读取 CSV 文件...\")\n",
    "    sim_dfs = []\n",
    "    real_df = None\n",
    "\n",
    "    for key, path in csv_path_dict.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"  [警告] 文件不存在: {path}\")\n",
    "            continue\n",
    "            \n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # 简单清洗：去除特征列为空的行\n",
    "        df = df.dropna(subset=config.FEATURE_COLS)\n",
    "\n",
    "        if key == 0: # Key 0 是真实数据 (Target Domain)\n",
    "            real_df = df\n",
    "            print(f\"  -> 已加载真实数据 (Target): {len(df)} 行\")\n",
    "        else:        # 其他 Key 是仿真数据 (Source Domain)\n",
    "            sim_dfs.append(df)\n",
    "    \n",
    "    if not sim_dfs or real_df is None:\n",
    "        raise ValueError(\"数据加载失败，请检查路径配置。\")\n",
    "\n",
    "    # 合并所有仿真数据\n",
    "    sim_df_all = pd.concat(sim_dfs, ignore_index=True)\n",
    "    print(f\"  -> 已合并仿真数据 (Source): {len(sim_df_all)} 行\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # B. 归一化 (Normalization)\n",
    "    # ---------------------------------------------------------\n",
    "    # 注意：必须在生成序列之前，对 DataFrame 的列进行归一化。\n",
    "    # 由于真实数据的 Pseudorange Residual 极其巨大 (10^7)，必须分别归一化\n",
    "    print(\"\\n[2/4] 正在进行归一化处理...\")\n",
    "\n",
    "    # 1. 归一化源域 (Simulation)\n",
    "    scaler_sim = MinMaxScaler()\n",
    "    sim_df_all[config.FEATURE_COLS] = scaler_sim.fit_transform(sim_df_all[config.FEATURE_COLS])\n",
    "\n",
    "    # 2. 归一化目标域 (Real World)\n",
    "    # 单独 fit 目标域，以将巨大的残差映射到 [0, 1] 范围，方便 LSTM 提取相对特征\n",
    "    scaler_real = MinMaxScaler()\n",
    "    real_df[config.FEATURE_COLS] = scaler_real.fit_transform(real_df[config.FEATURE_COLS])\n",
    "\n",
    "    print(\"  -> 归一化完成 (源域和目标域独立缩放)\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # C. 创建序列 (Make Sequences) - 调用你的函数\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[3/4] 正在调用 make_sequences 生成时序数据...\")\n",
    "\n",
    "    # 1. 源域序列 (带标签)\n",
    "    print(\"  -> 处理源域数据:\")\n",
    "    X_sim, y_sim = make_sequences(\n",
    "        df=sim_df_all,\n",
    "        features=config.FEATURE_COLS,\n",
    "        target=config.TARGET_COL,\n",
    "        seq_len=config.SEQ_LEN,\n",
    "        step=config.STEP,\n",
    "        group_col=config.GROUP_COL,\n",
    "        time_col=config.TIME_COL\n",
    "    )\n",
    "\n",
    "    # 2. 目标域序列 (无标签 / 不使用标签)\n",
    "    # DANN 中目标域通常被视为无标签，或者我们不传入 target 让函数只返回 X\n",
    "    print(\"  -> 处理目标域数据:\")\n",
    "    X_real = make_sequences(\n",
    "        df=real_df,\n",
    "        features=config.FEATURE_COLS,\n",
    "        target=None, # 目标域不提取标签用于训练 (或者你可以提取用于测试验证)\n",
    "        seq_len=config.SEQ_LEN,\n",
    "        step=config.STEP,\n",
    "        group_col=config.GROUP_COL,\n",
    "        time_col=config.TIME_COL\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # D. 创建 Dataset 和 DataLoader - 使用你的类\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[4/4] 创建 PyTorch DataLoaders...\")\n",
    "\n",
    "    # 1. 源域 Dataset\n",
    "    source_dataset = SeqDataset(X_sim, y_sim)\n",
    "    \n",
    "    # 2. 目标域 Dataset (没有标签，y会自动填0)\n",
    "    target_dataset = SeqDataset(X_real, y=None)\n",
    "\n",
    "    # 3. DataLoader\n",
    "    # drop_last=True 非常关键！因为 DANN 训练时如果不丢弃最后一个不完整的 batch，\n",
    "    # 可能导致源域和目标域 batch 尺寸不匹配报错。\n",
    "    source_loader = DataLoader(\n",
    "        source_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    target_loader = DataLoader(\n",
    "        target_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== 完成 ===\")\n",
    "    print(f\"源域 DataLoader: {len(source_loader)} batches\")\n",
    "    print(f\"目标域 DataLoader: {len(target_loader)} batches\")\n",
    "    \n",
    "    return source_loader, target_loader\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 执行\n",
    "# ---------------------------------------------------------\n",
    "# 确保这一步之前已经运行了你的 import os ... csv_path 代码\n",
    "source_loader, target_loader = get_dataloaders(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
