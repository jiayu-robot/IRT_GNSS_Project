{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a19734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Config] 正在使用的设备: cpu ---\n",
      "[load_data] 源域数据形状: (710063, 13), 目标域数据形状: (12614, 14)\n",
      "\n",
      "--- [序列制作] 正在创建 SVM 所需的序列... ---\n",
      "  - 正在处理源域 (仿真) 数据...\n",
      "  - X 形状: (709999, 5, 4), y 形状: (709999,), S 形状: (709999,)\n",
      "\n",
      "  - 正在处理目标域 (真实) 数据 (无标签处理)...\n",
      "  - X 形状: (12538, 5, 4), y 形状: (0,), S 形状: (12538,)\n",
      "--- 序列制作完成 ---\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from tqdm import tqdm\n",
    "from utility_uad_svm import create_sequences_for_svm, load_data, create_dataloaders\n",
    "from utility_uad_svm import get_features, complex_balance_data, standardize_features\n",
    "\n",
    "#加载所有参数 \n",
    "SRC_IDS = config.SRC_IDS # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "TGT_ID = config.TGT_ID    # 0\n",
    "CSV_PATHS = config.CSV_PATHS # 包含了所有路径的字典\n",
    "df_src, df_tgt = load_data(SRC_IDS, TGT_ID, CSV_PATHS)\n",
    "X_src, y_src, S_src, I_src_raw, X_tgt, y_tgt, S_tgt, TGT_INDICES = create_sequences_for_svm(df_src, df_tgt, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5a6f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [步骤 1] 正在加载 G_f 模型... ---\n"
     ]
    }
   ],
   "source": [
    "# （在你成功制作序列后执行此代码块）\n",
    "\n",
    "# --- 步骤 1: 加载训练好的 G_f 特征提取器 ---\n",
    "print(\"\\n--- [步骤 1] 正在加载 G_f 模型... ---\")\n",
    "from models import LSTMFeatureExtractor # 确保导入了模型定义\n",
    "from utility_uad_svm import get_features, complex_balance_data, standardize_features\n",
    "import os\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# 确保导入了 NumPy 和 Torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e21f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 正在执行特征提取...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - 提取特征中: 100%|██████████| 5547/5547 [01:07<00:00, 81.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 特征提取完毕。输出形状: (709999, 256)\n",
      "  - 正在执行特征提取...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - 提取特征中: 100%|██████████| 98/98 [00:01<00:00, 88.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 特征提取完毕。输出形状: (12538, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# F_src 和 F_tgt 是原始的、未缩放的特征向量\n",
    "device = torch.device(config.DEVICE)\n",
    "G_f = LSTMFeatureExtractor(\n",
    "    input_size=config.LSTM_INPUT_SIZE,\n",
    "    hidden_size=config.LSTM_HIDDEN_SIZE,\n",
    "    num_layers=config.LSTM_NUM_LAYERS,\n",
    "    dropout=config.LSTM_DROPOUT\n",
    ").to(device)\n",
    "\n",
    "G_f_path = os.path.join(config.MODEL_SAVE_DIR, \"G_f_final.pth\")\n",
    "G_f.load_state_dict(torch.load(G_f_path, map_location=device))\n",
    "G_f.eval()\n",
    "\n",
    "F_src_raw = get_features(G_f, X_src, config.BATCH_SIZE, device) \n",
    "F_tgt_raw = get_features(G_f, X_tgt, config.BATCH_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09efd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 在标准化之后，预测之前，强制清洗 NaN\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def clean_features(F_feat):\n",
    "    \"\"\"\n",
    "    清除特征中的 NaN 和 Inf 值。\n",
    "    \"\"\"\n",
    "    # 查找并计数 NaN\n",
    "    nan_count = np.sum(np.isnan(F_feat))\n",
    "    inf_count = np.sum(np.isinf(F_feat))\n",
    "    \n",
    "    if nan_count > 0 or inf_count > 0:\n",
    "        print(f\"【警告】发现 {nan_count} 个 NaN 和 {inf_count} 个 Inf。正在执行清理...\")\n",
    "        \n",
    "        # 将所有 NaN/Inf 替换为 0 (或其他合适的默认值)\n",
    "        # 另一种方法是使用特征的均值，但简单替换为 0 更安全，因为特征已经标准化（均值接近 0）\n",
    "        F_feat = np.nan_to_num(F_feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        print(\"清理完成。\")\n",
    "    return F_feat\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ！！在你脚本中 Step 1/2 之后，插入以下调用：\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# F_src_scaled, F_tgt_scaled = standardize_features(F_src_raw, F_tgt_raw) \n",
    "# ... 标准化代码之后 ...\n",
    "\n",
    "# 立即清洗特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6ab0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [关键修正] 正在执行标准化前数据清洗... ---\n",
      "【警告】发现 1352448 个 NaN 和 0 个 Inf。正在执行清理...\n",
      "清理完成。\n",
      "\n",
      "--- [标准化] 正在对特征进行标准化... ---\n",
      "  - 特征标准化完成 (基于源域统计量)。\n",
      " - F_src_scaled 形状: (709999, 256)\n",
      " - F_tgt_scaled 形状: (12538, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- [关键修正] 正在执行标准化前数据清洗... ---\")\n",
    "\n",
    "# 1. 强制清洗源域特征 (F_src_raw) - 避免 Scaler 污染\n",
    "F_src_raw = clean_features(F_src_raw) \n",
    "# 2. 强制清洗目标域特征 (F_tgt_raw) - 避免其原始 Inf/NaN 影响 transform \n",
    "F_tgt_raw = clean_features(F_tgt_raw) \n",
    "\n",
    "# --- [步骤 1] 正在进行特征标准化... ---\n",
    "# 现在 standardize_features 可以安全运行，因为它接收的是干净的 F_src_raw\n",
    "F_src_scaled, F_tgt_scaled = standardize_features(F_src_raw, F_tgt_raw) \n",
    "# (你的 standardize_features 内部已包含 sigma=0 修复)\n",
    "\n",
    "print(f\" - F_src_scaled 形状: {F_src_scaled.shape}\")\n",
    "print(f\" - F_tgt_scaled 形状: {F_tgt_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0cab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [步骤 2] 正在对源域数据进行分组和精细平衡... ---\n",
      "\n",
      "--- 平衡 GPS 组数据 (LOS/NLOS) ---\n",
      "  - 原始类别分布: Counter({np.int64(1): 343282, np.int64(0): 102860})\n",
      "✅ 特征空间精细平衡完成。\n",
      " - GPS LOS/NLOS 各: 102860 条\n",
      " - PL LOS/NLOS 各: 0 条\n",
      " - 平衡后总样本数: 205720\n",
      "\n",
      "--- 平衡 PL 组数据 (LOS/NLOS) ---\n",
      "  - 原始类别分布: Counter({np.int64(0): 230541, np.int64(1): 33316})\n",
      "✅ 特征空间精细平衡完成。\n",
      " - GPS LOS/NLOS 各: 0 条\n",
      " - PL LOS/NLOS 各: 33316 条\n",
      " - 平衡后总样本数: 66632\n"
     ]
    }
   ],
   "source": [
    "# --- 步骤 2: 源域数据分组与精细平衡 ---\n",
    "print(\"\\n--- [步骤 2] 正在对源域数据进行分组和精细平衡... ---\")\n",
    "\n",
    "# 2a. 按卫星类型分组 (使用缩放后的特征)\n",
    "idx_src_gps = np.where(S_src == 0)[0] # GPS 卫星的索引\n",
    "idx_src_pl = np.where(S_src == 1)[0]  # PL 卫星的索引\n",
    "\n",
    "# 分组缩放后的特征 F_src_scaled\n",
    "F_src_gps = F_src_scaled[idx_src_gps]\n",
    "y_src_gps = y_src[idx_src_gps]\n",
    "S_src_gps = S_src[idx_src_gps] # 传入 GPS 组的 Sat_Type\n",
    "\n",
    "F_src_pl = F_src_scaled[idx_src_pl]\n",
    "y_src_pl = y_src[idx_src_pl]\n",
    "S_src_pl = S_src[idx_src_pl] # 传入 PL 组的 Sat_Type\n",
    "\n",
    "\n",
    "# 2b. 平衡 GPS 组\n",
    "print(\"\\n--- 平衡 GPS 组数据 (LOS/NLOS) ---\")\n",
    "# F_gps_bal, y_gps_bal 是平衡后的 GPS 训练数据\n",
    "F_gps_bal, y_gps_bal = complex_balance_data(F_src_gps, y_src_gps, S_src_gps, random_state=42)\n",
    "\n",
    "# 2c. 平衡 PL 组\n",
    "print(\"\\n--- 平衡 PL 组数据 (LOS/NLOS) ---\")\n",
    "# F_pl_bal, y_pl_bal 是平衡后的 PL 训练数据\n",
    "F_pl_bal, y_pl_bal = complex_balance_data(F_src_pl, y_src_pl, S_src_pl, random_state=42)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 此时，你得到了最终用于训练的数据：F_gps_bal, y_gps_bal, F_pl_bal, y_pl_bal\n",
    "# ----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3fe47d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [步骤 2.5] 对平衡后的数据进行二次欠采样 (控制 SVM 训练规模) ---\n",
      " - GPS 训练集规模已从 205720 减少到 20000。\n",
      " - PL 训练集规模已从 66632 减少到 20000。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 定义每个专家模型的最大样本数\n",
    "MAX_SVM_SAMPLES = 20000 # <-- 确保这里是 20000\n",
    "\n",
    "print(\"\\n--- [步骤 2.5] 对平衡后的数据进行二次欠采样 (控制 SVM 训练规模) ---\")\n",
    "\n",
    "# --- GPS 专家模型二次采样 ---\n",
    "if len(F_gps_bal) > MAX_SVM_SAMPLES:\n",
    "    # 修正：我们不使用 train_test_split，而是使用 train_test_split 来获得一个 20k 的子集\n",
    "    \n",
    "    # 1. 使用 train_test_split 获得训练集部分（20k）\n",
    "    F_gps_keep, _, y_gps_keep, _ = train_test_split(\n",
    "        F_gps_bal, y_gps_bal, \n",
    "        train_size=MAX_SVM_SAMPLES, # 返回数组 F_gps_keep 的大小是 20000\n",
    "        random_state=42,\n",
    "        stratify=y_gps_bal \n",
    "    )\n",
    "    \n",
    "    print(f\" - GPS 训练集规模已从 {len(F_gps_bal)} 减少到 {len(F_gps_keep)}。\")\n",
    "    F_gps_bal = F_gps_keep\n",
    "    y_gps_bal = y_gps_keep\n",
    "else:\n",
    "    print(f\" - GPS 训练集 ({len(F_gps_bal)} 样本) 小于 {MAX_SVM_SAMPLES}，无需采样。\")\n",
    "\n",
    "\n",
    "# --- PL 专家模型二次采样 ---\n",
    "if len(F_pl_bal) > MAX_SVM_SAMPLES:\n",
    "    # 1. 使用 train_test_split 获得训练集部分（20k）\n",
    "    F_pl_keep, _, y_pl_keep, _ = train_test_split(\n",
    "        F_pl_bal, y_pl_bal, \n",
    "        train_size=MAX_SVM_SAMPLES, # 返回数组 F_pl_keep 的大小是 20000\n",
    "        random_state=42,\n",
    "        stratify=y_pl_bal \n",
    "    )\n",
    "    \n",
    "    print(f\" - PL 训练集规模已从 {len(F_pl_bal)} 减少到 {len(F_pl_keep)}。\")\n",
    "    F_pl_bal = F_pl_keep\n",
    "    y_pl_bal = y_pl_keep\n",
    "else:\n",
    "    print(f\" - PL 训练集 ({len(F_pl_bal)} 样本) 小于 {MAX_SVM_SAMPLES}，无需采样。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de2c135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [步骤 3] 正在训练 GPS 和 PL 专家模型... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67aedce081294528b3b9f34d0f6346b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练 SVM_GPS 专家模型:   0%|                              | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - SVM_GPS 训练完成。训练样本数: 20000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5025d6e4b30542fdb4d809f9341036cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练 SVM_PL 专家模型:   0%|                               | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - SVM_PL 训练完成。训练样本数: 20000\n",
      "\n",
      "--- [SVM 训练集性能诊断] ---\n",
      "\n",
      "--- [步骤 6] 正在保存 SVM 专家模型... ---\n",
      " - SVM_GPS 专家模型已保存到: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\SVM\\checkpoints\\svm_gps_expert.pkl\n",
      " - SVM_PL 专家模型已保存到: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\SVM\\checkpoints\\svm_pl_expert.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# 确保在你的脚本顶部导入了 tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os \n",
    "from sklearn.svm import LinearSVC # 需要导入新的类\n",
    "# 假设 svm_gps, svm_pl, F_tgt_scaled, S_tgt, y_tgt, config 等变量已在内存中\n",
    "\n",
    "# --- 步骤 3: 训练双 SVM 专家模型 ---\n",
    "print(\"\\n--- [步骤 3] 正在训练 GPS 和 PL 专家模型... ---\")\n",
    "\n",
    "# 3a. 训练 GPS 专家模型\n",
    "svm_gps = SVC(kernel='rbf', C=100.0, class_weight='balanced', random_state=42)\n",
    "if len(F_gps_bal) > 0:\n",
    "    for _ in tqdm(range(1), desc=\"训练 SVM_GPS 专家模型\", ncols=80):\n",
    "        svm_gps.fit(F_gps_bal, y_gps_bal)\n",
    "    print(f\" - SVM_GPS 训练完成。训练样本数: {len(F_gps_bal)}\")\n",
    "else:\n",
    "    print(\" - 警告：SVM_GPS 样本不足，跳过训练。\")\n",
    "\n",
    "# 3b. 训练 PL 专家模型\n",
    "svm_pl = SVC(kernel='rbf', C=100.0, class_weight='balanced', random_state=42)\n",
    "if len(F_pl_bal) > 0:\n",
    "    for _ in tqdm(range(1), desc=\"训练 SVM_PL 专家模型\", ncols=80):\n",
    "        svm_pl.fit(F_pl_bal, y_pl_bal)\n",
    "    print(f\" - SVM_PL 训练完成。训练样本数: {len(F_pl_bal)}\")\n",
    "else:\n",
    "    print(\" - 警告：SVM_PL 样本不足，跳过训练。\")\n",
    "# 假设 svm_gps 和 svm_pl 对象已在内存中\n",
    "\n",
    "# (在 SVM_GPS 和 SVM_PL 训练完成后)\n",
    "\n",
    "print(\"\\n--- [SVM 训练集性能诊断] ---\")\n",
    "\n",
    "# --- 步骤 6: 保存 SVM 专家模型 ---\n",
    "print(\"\\n--- [步骤 6] 正在保存 SVM 专家模型... ---\")\n",
    "\n",
    "# 确保保存目录存在\n",
    "os.makedirs(config.MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 定义保存路径，使用 config 中已有的 MODEL_SAVE_DIR\n",
    "gps_path = os.path.join(config.MODEL_SAVE_DIR, \"svm_gps_expert.pkl\")\n",
    "pl_path = os.path.join(config.MODEL_SAVE_DIR, \"svm_pl_expert.pkl\")\n",
    "\n",
    "# 保存 SVM 模型\n",
    "joblib.dump(svm_gps, gps_path)\n",
    "joblib.dump(svm_pl, pl_path)\n",
    "\n",
    "print(f\" - SVM_GPS 专家模型已保存到: {gps_path}\")\n",
    "print(f\" - SVM_PL 专家模型已保存到: {pl_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd31cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [步骤 4] 目标域预测 (路由中)... ---\n",
      " - 预测结果重组完成。\n",
      "\n",
      "--- [步骤 5] 评估和报告 ---\n",
      "\n",
      "=======================================================\n",
      "✅ 预测完成。目标域无标签，跳过评估。\n",
      "已生成 12538 条预测结果 (y_pred_tgt)。\n",
      "=======================================================\n",
      "\n",
      "--- [步骤 6] 预测结果回溯合并与保存 ---\n",
      "✅ 最终结果文件已成功保存到: c:\\Users\\yangj\\Desktop\\4JYY\\4JYY\\Transfer Learning\\SVM\\checkpoints\\results\\tgt_data_with_predictions.csv\n",
      "文件已包含所有原始列，并在末尾附加了 'predicted_multipath' 列。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# --- 假设以下变量和函数已在内存中并已准备好 ---\n",
    "# F_tgt_scaled: 目标域标准化特征 (来自 standardize_features)\n",
    "# S_tgt: 目标域卫星类型数组\n",
    "# y_tgt: 目标域标签数组 (可能是空的占位符)\n",
    "# config: 配置对象\n",
    "# df_tgt: 原始目标域 DataFrame\n",
    "# TGT_INDICES: 原始索引数组 (来自 create_sequences_for_svm)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# --- 警告: 请确保在运行此块代码之前，已加载或训练了 SVM 模型 ---\n",
    "try:\n",
    "    # 加载已训练的专家模型 (假设它们在 config.MODEL_SAVE_DIR/checkpoints 路径下)\n",
    "    MODEL_DIR = config.MODEL_SAVE_DIR\n",
    "    svm_gps = joblib.load(os.path.join(MODEL_DIR, \"svm_gps_expert.pkl\"))\n",
    "    svm_pl  = joblib.load(os.path.join(MODEL_DIR, \"svm_pl_expert.pkl\"))\n",
    "    \n",
    "    # 假设 F_gps_bal 和 F_pl_bal 至少是空数组，以进行安全检查\n",
    "    F_gps_bal = np.array([]) \n",
    "    F_pl_bal = np.array([]) \n",
    "    # 在实际训练脚本中，这两个数组会通过 complex_balance_data 赋值。\n",
    "    # 这里我们只检查长度，避免报错。\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"致命错误：无法加载 SVM 模型或初始化 F_bal 数组。请先运行完整的训练脚本。错误: {e}\")\n",
    "    # exit()\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# --- 步骤 4: 目标域预测与路由 ---\n",
    "print(\"\\n--- [步骤 4] 目标域预测 (路由中)... ---\")\n",
    "\n",
    "# 4a. 目标域分组 (路由)\n",
    "idx_tgt_gps = np.where(S_tgt == 0)[0] \n",
    "idx_tgt_pl = np.where(S_tgt == 1)[0] \n",
    "\n",
    "F_tgt_gps = F_tgt_scaled[idx_tgt_gps]\n",
    "F_tgt_pl = F_tgt_scaled[idx_tgt_pl]\n",
    "\n",
    "# 4b. 预测和重组\n",
    "y_pred_tgt = np.zeros(len(F_tgt_scaled), dtype=np.int64) \n",
    "\n",
    "# GPS 预测 (仅在模型训练和数据存在时执行)\n",
    "if len(F_gps_bal) > 0 and len(F_tgt_gps) > 0:\n",
    "    with tqdm(total=len(F_tgt_gps), desc=\"  - GPS 专家预测中\", leave=False) as pbar:\n",
    "        y_pred_gps = svm_gps.predict(F_tgt_gps)\n",
    "        y_pred_tgt[idx_tgt_gps] = y_pred_gps\n",
    "        pbar.update(len(F_tgt_gps))\n",
    "    print(f\" - GPS 目标域预测完成。样本数: {len(F_tgt_gps)}\")\n",
    "\n",
    "# PL 预测\n",
    "if len(F_pl_bal) > 0 and len(F_tgt_pl) > 0:\n",
    "    with tqdm(total=len(F_tgt_pl), desc=\"  - PL 专家预测中\", leave=False) as pbar:\n",
    "        y_pred_pl = svm_pl.predict(F_tgt_pl)\n",
    "        y_pred_tgt[idx_tgt_pl] = y_pred_pl\n",
    "        pbar.update(len(F_tgt_pl))\n",
    "    print(f\" - PL 目标域预测完成。样本数: {len(F_tgt_pl)}\")\n",
    "\n",
    "print(\" - 预测结果重组完成。\")\n",
    "\n",
    "\n",
    "# --- 步骤 5: 评估和报告 ---\n",
    "print(\"\\n--- [步骤 5] 评估和报告 ---\")\n",
    "\n",
    "if len(y_tgt) > 0:\n",
    "    # ... (评估代码) ...\n",
    "    accuracy = accuracy_score(y_tgt, y_pred_tgt)\n",
    "    report = classification_report(y_tgt, y_pred_tgt, \n",
    "                                   target_names=[f'Class {i}' for i in range(config.NUM_CLASSES)], \n",
    "                                   output_dict=False, zero_division=0)\n",
    "    \n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"✅ DANN-LSTM 双 SVM 专家模型性能报告：\")\n",
    "    print(f\"目标域总体 SVM 准确率 (Accuracy): {accuracy:.4f}\")\n",
    "    print(\"目标域 SVM 评估报告 (GPS/PL 路由后的综合性能):\\n\", report)\n",
    "    print(\"=======================================================\")\n",
    "else:\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"✅ 预测完成。目标域无标签，跳过评估。\")\n",
    "    print(f\"已生成 {len(y_pred_tgt)} 条预测结果 (y_pred_tgt)。\")\n",
    "    print(\"=======================================================\")\n",
    "\n",
    "\n",
    "# --- 步骤 6: 预测结果回溯合并与保存 ---\n",
    "print(\"\\n--- [步骤 6] 预测结果回溯合并与保存 ---\")\n",
    "\n",
    "if 'df_tgt' in locals() and 'TGT_INDICES' in locals():\n",
    "    \n",
    "    # 1. 创建一个与原始 df_tgt 长度相同的空预测列\n",
    "    full_predictions = pd.Series(\n",
    "        data=np.nan, \n",
    "        index=df_tgt.index, \n",
    "        dtype=float \n",
    "    )\n",
    "\n",
    "    # 2. ！！关键的回溯操作：使用 TGT_INDICES 映射预测值 ！！\n",
    "    full_predictions.loc[TGT_INDICES] = y_pred_tgt\n",
    "\n",
    "    # 3. 将新的预测列附加到原始 DataFrame\n",
    "    df_tgt['predicted_multipath'] = full_predictions\n",
    "\n",
    "    # 4. 定义输出并保存文件\n",
    "    OUTPUT_DIR = os.path.join(config.MODEL_SAVE_DIR, \"results\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"tgt_data_with_predictions.csv\")\n",
    "\n",
    "    df_tgt.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "    print(f\"✅ 最终结果文件已成功保存到: {OUTPUT_PATH}\")\n",
    "    print(\"文件已包含所有原始列，并在末尾附加了 'predicted_multipath' 列。\")\n",
    "else:\n",
    "    print(\"警告：缺少 df_tgt 或 TGT_INDICES 变量，跳过结果文件保存。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
