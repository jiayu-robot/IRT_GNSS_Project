{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0179317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Config] æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡: cpu ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# å¼•å…¥é…ç½®å’Œæ¨¡å‹\n",
    "import config \n",
    "# ç¡®ä¿ models é‡Œçš„ LSTMFeatureExtractor æ˜¯æœ€æ–°çš„ï¼ˆå¸¦ fc_final çš„ç‰ˆæœ¬ï¼‰\n",
    "from models import LSTMFeatureExtractor \n",
    "from utility_uad_svm import load_data, create_sequences_for_svm, get_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df43b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(F_feat):\n",
    "    \"\"\"æ¸…é™¤ç‰¹å¾ä¸­çš„ NaN å’Œ Inf å€¼ã€‚\"\"\"\n",
    "    nan_count = np.sum(np.isnan(F_feat))\n",
    "    inf_count = np.sum(np.isinf(F_feat))\n",
    "    if nan_count > 0 or inf_count > 0:\n",
    "        print(f\"ã€âš ï¸ æ¸…ç†ã€‘å‘ç° {nan_count} NaN / {inf_count} Infã€‚æ­£åœ¨ä¿®å¤...\")\n",
    "        F_feat = np.nan_to_num(F_feat, nan=0.0, posinf=1e8, neginf=-1e8) \n",
    "    return F_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceffd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_balance_data(X_feat, y_labels, sat_types, random_state=42):\n",
    "    \"\"\"\n",
    "    ã€å…³é”®ä¿®æ”¹ã€‘åŸºäº æ ‡ç­¾(y) å’Œ å«æ˜Ÿç±»å‹(sat_type) è¿›è¡Œå››è±¡é™å¹³è¡¡ã€‚\n",
    "    ç¡®ä¿ GNSS/PL çš„ å¥½/å ä¿¡å·åœ¨ SVM è®­ç»ƒé›†ä¸­æ¯”ä¾‹ä¸€è‡´ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"  - [å¹³è¡¡å‰] æ€»ä½“åˆ†å¸ƒ: {Counter(y_labels)}\")\n",
    "    \n",
    "    # æ„å»ºä¸´æ—¶ DataFrame æ–¹ä¾¿ç´¢å¼•\n",
    "    df_temp = pd.DataFrame({\n",
    "        'idx': range(len(X_feat)), \n",
    "        'y': y_labels, \n",
    "        'sat': sat_types\n",
    "    })\n",
    "    \n",
    "    # 1. åˆ’åˆ†å››ç±»äººç¾¤\n",
    "    # å‡è®¾ sat_type: 0=GNSS, 1=PL; y: 0=LOS(å¥½), 1=Multipath(å)\n",
    "    idx_gnss_good = df_temp[(df_temp['sat'] == 0) & (df_temp['y'] == 0)]['idx'].values\n",
    "    idx_gnss_bad  = df_temp[(df_temp['sat'] == 0) & (df_temp['y'] == 1)]['idx'].values\n",
    "    idx_pl_good   = df_temp[(df_temp['sat'] == 1) & (df_temp['y'] == 0)]['idx'].values\n",
    "    idx_pl_bad    = df_temp[(df_temp['sat'] == 1) & (df_temp['y'] == 1)]['idx'].values\n",
    "    \n",
    "    print(f\"  - ç»„åˆ«ç»Ÿè®¡: GNSSå¥½:{len(idx_gnss_good)}, GNSSå:{len(idx_gnss_bad)}, PLå¥½:{len(idx_pl_good)}, PLå:{len(idx_pl_bad)}\")\n",
    "    \n",
    "    # 2. æ‰¾åˆ°çŸ­æ¿ï¼ˆæœ€å°æ•°é‡ï¼‰\n",
    "    # æ³¨æ„ï¼šå¦‚æœæŸä¸€ç»„æ˜¯ 0ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†\n",
    "    counts = [len(idx_gnss_good), len(idx_gnss_bad), len(idx_pl_good), len(idx_pl_bad)]\n",
    "    counts = [c for c in counts if c > 0] # è¿‡æ»¤æ‰ 0\n",
    "    if not counts:\n",
    "        return np.empty((0, X_feat.shape[1])), np.empty((0,))\n",
    "        \n",
    "    min_size = min(counts)\n",
    "    print(f\"  - å¹³è¡¡åŸºå‡†æ•° (min_size): {min_size}\")\n",
    "    \n",
    "    \n",
    "    # 3. æ¬ é‡‡æ ·å‡½æ•°\n",
    "    def sample_indices(indices, n):\n",
    "        if len(indices) == 0: return indices\n",
    "        np.random.seed(random_state)\n",
    "        return np.random.choice(indices, n, replace=False)\n",
    "\n",
    "    # 4. æ‰§è¡Œé‡‡æ ·\n",
    "    final_indices = np.concatenate([\n",
    "        sample_indices(idx_gnss_good, min_size),\n",
    "        sample_indices(idx_gnss_bad, min_size),\n",
    "        sample_indices(idx_pl_good, min_size),\n",
    "        sample_indices(idx_pl_bad, min_size)\n",
    "    ])\n",
    "    \n",
    "    np.random.shuffle(final_indices)\n",
    "    \n",
    "    return X_feat[final_indices], y_labels[final_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7cb3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_features_global(F_src, F_tgt):\n",
    "#     \"\"\"\n",
    "#     å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ã€‚\n",
    "#     é€»è¾‘ï¼šåœ¨æºåŸŸä¸Š Fitï¼Œåº”ç”¨åˆ°æºåŸŸå’Œç›®æ ‡åŸŸã€‚\n",
    "#     \"\"\"\n",
    "#     print(\"  - æ‰§è¡Œç‰¹å¾æ ‡å‡†åŒ– (Z-Score)...\")\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(F_src) # åªåœ¨æºåŸŸä¸Šå­¦ä¹ å‡å€¼å’Œæ–¹å·®\n",
    "    \n",
    "#     # é˜²å¾¡æ ‡å‡†å·®ä¸º 0\n",
    "#     zero_std_mask = scaler.scale_ == 0\n",
    "#     if np.any(zero_std_mask):\n",
    "#         scaler.scale_[zero_std_mask] = 1e-8\n",
    "        \n",
    "#     F_src_scaled = scaler.transform(F_src)\n",
    "#     F_tgt_scaled = scaler.transform(F_tgt)\n",
    "#     return F_src_scaled, F_tgt_scaled\n",
    "\n",
    "def standardize_features_global(F_src, F_tgt):\n",
    "    print(\"  - æ‰§è¡Œç‰¹å¾æ ‡å‡†åŒ– (Z-Score)...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(F_src) # è®¡ç®—æºåŸŸçš„å‡å€¼å’Œæ–¹å·®\n",
    "    \n",
    "    # --- ğŸ” å¼ºåŠ›è°ƒè¯•ä¸ä¿®å¤ ---\n",
    "    print(f\"  - [è°ƒè¯•] ç‰¹å¾å‡å€¼å‰5ä½: {scaler.mean_[:5]}\")\n",
    "    print(f\"  - [è°ƒè¯•] ç‰¹å¾æ–¹å·®å‰5ä½: {scaler.var_[:5]}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ–¹å·®æ˜¯å¦æå° (æ¥è¿‘0)\n",
    "    # æ³¨æ„ï¼šStandardScaler å†…éƒ¨ç”¨ var_, å¦‚æœ var_ ä¸º 0ï¼Œscale_ ä¹Ÿä¼šå‡ºé—®é¢˜\n",
    "    zero_var_mask = scaler.var_ < 1e-8 # ä½¿ç”¨ä¸€ä¸ªæå°é˜ˆå€¼\n",
    "    if np.any(zero_var_mask):\n",
    "        num_dead = np.sum(zero_var_mask)\n",
    "        print(f\"ã€âš ï¸ ä¸¥é‡è­¦å‘Šã€‘å‘ç° {num_dead} ä¸ªç‰¹å¾ç»´åº¦æ–¹å·®å‡ ä¹ä¸º 0 (æ­»ç¥ç»å…ƒ)ï¼\")\n",
    "        # å¼ºåˆ¶æŠŠè¿™äº›ç»´åº¦çš„æ–¹å·®è®¾ä¸º 1ï¼Œé˜²æ­¢é™¤ä»¥ 0 (è¿™æ · x-mean åç»“æœå°±æ˜¯ 0ï¼Œä¸ä¼šæ˜¯ NaN)\n",
    "        scaler.var_[zero_var_mask] = 1.0 \n",
    "        scaler.scale_[zero_var_mask] = 1.0 \n",
    "    \n",
    "    F_src_scaled = scaler.transform(F_src)\n",
    "    F_tgt_scaled = scaler.transform(F_tgt)\n",
    "    \n",
    "    return F_src_scaled, F_tgt_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c154d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_IDS = config.SRC_IDS\n",
    "TGT_ID = config.TGT_ID\n",
    "CSV_PATHS = config.CSV_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4635c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SVM Main] æ­£åœ¨ä½¿ç”¨è®¾å¤‡: cpu ---\n",
      "\n",
      "--- [æ­¥éª¤ 1] åŠ è½½åŸå§‹æ•°æ® ---\n",
      "[load_data] æ­£åœ¨åŠ è½½ç›®æ ‡åŸŸæ•°æ®: 0\n",
      "[load_data] ç›®æ ‡åŸŸåŠ è½½å®Œæˆã€‚å½¢çŠ¶: (12614, 14)\n",
      "[load_data] æ³¨ï¼šæºåŸŸæ•°æ®å°†åœ¨ 'create_all_sequences' ä¸­é€ä¸ªåŠ è½½ä»¥ä¿è¯æ—¶åºç‹¬ç«‹æ€§ã€‚\n",
      "\n",
      "--- [æ­¥éª¤ 2] åˆ¶ä½œåºåˆ— ---\n",
      "\n",
      "--- [åºåˆ—åˆ¶ä½œ] æ­£åœ¨åˆ›å»º SVM æ‰€éœ€çš„åºåˆ— (é€ä¸ªCaseå¤„ç†)... ---\n",
      "  - å¤„ç†æºåŸŸ Case 1: Case1_Urban_10Hz_with_sat_type.csv\n",
      " Â - X å½¢çŠ¶: (91575, 10, 5), y å½¢çŠ¶: (91575,), S å½¢çŠ¶: (91575,)\n",
      "  - å¤„ç†æºåŸŸ Case 2: Case1_Suburban_10Hz_with_sat_type.csv\n",
      " Â - X å½¢çŠ¶: (91575, 10, 5), y å½¢çŠ¶: (91575,), S å½¢çŠ¶: (91575,)\n",
      "  - å¤„ç†æºåŸŸ Case 3: Case2_Urban_10Hz_with_sat_type.csv\n",
      " Â - X å½¢çŠ¶: (79712, 10, 5), y å½¢çŠ¶: (79712,), S å½¢çŠ¶: (79712,)\n",
      "  - å¤„ç†æºåŸŸ Case 4: Case2_Suburban_10Hz_with_sat_type.csv\n",
      " Â - X å½¢çŠ¶: (79750, 10, 5), y å½¢çŠ¶: (79750,), S å½¢çŠ¶: (79750,)\n",
      "  - å¤„ç†æºåŸŸ Case 5: Case3_Urban_10Hz_with_sat_type.csv\n",
      " Â - X å½¢çŠ¶: (91574, 10, 5), y å½¢çŠ¶: (91574,), S å½¢çŠ¶: (91574,)\n",
      "  - å¤„ç†æºåŸŸ Case 6: Case3_Suburban_10Hz_with_sat_type.csv\n",
      " Â - X å½¢çŠ¶: (91575, 10, 5), y å½¢çŠ¶: (91575,), S å½¢çŠ¶: (91575,)\n",
      "  - å¤„ç†æºåŸŸ Case 7: Case4_Urban_10Hz_with_sat_type.csv\n",
      " Â - X å½¢çŠ¶: (91575, 10, 5), y å½¢çŠ¶: (91575,), S å½¢çŠ¶: (91575,)\n",
      "  - å¤„ç†æºåŸŸ Case 8: Case4_Suburban_10Hz_with_sat_type.csv\n",
      " Â - X å½¢çŠ¶: (91575, 10, 5), y å½¢çŠ¶: (91575,), S å½¢çŠ¶: (91575,)\n",
      "  - æºåŸŸåˆå¹¶å®Œæˆã€‚æ€»æ•°: 708911\n",
      "\n",
      "  - å¤„ç†ç›®æ ‡åŸŸ (çœŸå®) æ•°æ®...\n",
      " Â - X å½¢çŠ¶: (12443, 10, 5), y å½¢çŠ¶: (0,), S å½¢çŠ¶: (12443,)\n",
      "--- [æ•°æ®åˆ†å¸ƒæ£€æŸ¥] ---\n",
      "ç›®æ ‡åŸŸå«æ˜Ÿåˆ†å¸ƒ: Counter({np.int64(0): 9691, np.int64(-1): 1376, np.int64(1): 1376})\n",
      "\n",
      "--- [æ­¥éª¤ 3] åŠ è½½ G_f æ¨¡å‹å¹¶æå–ç‰¹å¾ ---\n",
      " Â - æ­£åœ¨æ‰§è¡Œç‰¹å¾æå–...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Â - æå–ç‰¹å¾ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11077/11077 [01:12<00:00, 152.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Â - ç‰¹å¾æå–å®Œæ¯•ã€‚è¾“å‡ºå½¢çŠ¶: (708911, 128)\n",
      "ã€âœ… æ­£å¸¸ã€‘åŸå§‹ç‰¹å¾é‡Œæ²¡æœ‰ NaNã€‚é—®é¢˜å‡ºåœ¨æ ‡å‡†åŒ–æ­¥éª¤ã€‚\n",
      " Â - æ­£åœ¨æ‰§è¡Œç‰¹å¾æå–...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Â - æå–ç‰¹å¾ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [00:01<00:00, 114.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Â - ç‰¹å¾æå–å®Œæ¯•ã€‚è¾“å‡ºå½¢çŠ¶: (12443, 128)\n",
      "  - æºåŸŸç‰¹å¾å½¢çŠ¶: (708911, 128)\n",
      "\n",
      "--- [æ­¥éª¤ 4] ç‰¹å¾æ ‡å‡†åŒ– ---\n",
      "  - æ‰§è¡Œç‰¹å¾æ ‡å‡†åŒ– (Z-Score)...\n",
      "  - [è°ƒè¯•] ç‰¹å¾å‡å€¼å‰5ä½: [0.59341562 0.77596106 0.33016311 0.36866778 0.40965743]\n",
      "  - [è°ƒè¯•] ç‰¹å¾æ–¹å·®å‰5ä½: [1.33573844 1.98276945 0.19791398 0.26715629 0.17905417]\n",
      "ã€âš ï¸ æ¸…ç†ã€‘å‘ç° 1240320 NaN / 0 Infã€‚æ­£åœ¨ä¿®å¤...\n",
      "\n",
      "--- [æ­¥éª¤ 5] æ•°æ®å¹³è¡¡ä¸ SVM è®­ç»ƒ ---\n",
      "  - [å¹³è¡¡å‰] æ€»ä½“åˆ†å¸ƒ: Counter({np.int64(1): 376006, np.int64(0): 332905})\n",
      "  - ç»„åˆ«ç»Ÿè®¡: GNSSå¥½:102694, GNSSå:342768, PLå¥½:230211, PLå:33238\n",
      "  - å¹³è¡¡åŸºå‡†æ•° (min_size): 33238\n",
      "  - æ•°æ®é‡ 132952 > 20000ï¼Œè¿›è¡ŒäºŒæ¬¡éšæœºé‡‡æ ·...\n",
      "  - æœ€ç»ˆ SVM è®­ç»ƒé›†å¤§å°: (20000, 128)\n",
      "  - å¼€å§‹è®­ç»ƒ SVM (RBF Kernel)...\n",
      "âœ… SVM è®­ç»ƒå®Œæˆã€‚\n",
      "\n",
      "--- [æ­¥éª¤ 6] ç›®æ ‡åŸŸé¢„æµ‹ ---\n",
      "  - ç›®æ ‡åŸŸæ— æœ‰æ•ˆæ ‡ç­¾ (æˆ–ä»…æœ‰ä¸€ç§æ ‡ç­¾)ï¼Œè·³è¿‡ Accuracy è¯„ä¼°ã€‚\n",
      "\n",
      "--- [æ­¥éª¤ 7] ä¿å­˜ç»“æœ ---\n",
      "âœ… ç»“æœå·²ä¿å­˜è‡³: c:\\Users\\yangj\\Desktop\\GNSS-main\\GNSS-main\\Transfer Learning\\SVM\\checkpoints\\results_single_svm\\tgt_data_with_single_svm_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ä¸»ç¨‹åº\n",
    "# =============================================================================\n",
    "\n",
    "def main_single_svm():\n",
    "    device = torch.device(config.DEVICE)\n",
    "    print(f\"--- [SVM Main] æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device} ---\")\n",
    "    SRC_IDS = config.SRC_IDS\n",
    "    TGT_ID = config.TGT_ID\n",
    "    CSV_PATHS = config.CSV_PATHS\n",
    "    # -------------------------------------------------------\n",
    "    # 1. åŠ è½½æ•°æ® & åŸå§‹æ•°æ®æ ‡å‡†åŒ– (å¿…é¡»ä¸ DANN è®­ç»ƒæ—¶ä¸€è‡´!)\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- [æ­¥éª¤ 1] åŠ è½½åŸå§‹æ•°æ® ---\")\n",
    "    df_src, df_tgt = load_data(SRC_IDS, TGT_ID, CSV_PATHS)\n",
    "    \n",
    "    # # ã€é‡è¦æç¤ºã€‘å¦‚æœ DANN è®­ç»ƒæ—¶å¯¹åŸå§‹æ•°æ®åšäº†æ ‡å‡†åŒ–ï¼Œè¿™é‡Œå¿…é¡»åšã€‚\n",
    "    # # å¦‚æœ DANN è®­ç»ƒæ—¶æ²¡åšï¼Œè¯·æ³¨é‡Šæ‰ä¸‹é¢è¿™å‡ è¡Œï¼\n",
    "    # print(\"  - [æç¤º] æ­£åœ¨å¯¹åŸå§‹è¾“å…¥ä¿¡å·è¿›è¡Œæ ‡å‡†åŒ– (å‡è®¾ DANN è®­ç»ƒæ—¶ä¹Ÿåšäº†)...\")\n",
    "    # raw_scaler = StandardScaler()\n",
    "    # # fit on source\n",
    "    # df_src[config.FEATURES] = raw_scaler.fit_transform(df_src[config.FEATURES])\n",
    "    # # transform target\n",
    "    # df_tgt[config.FEATURES] = raw_scaler.transform(df_tgt[config.FEATURES])\n",
    "    # print(\"âœ… åŸå§‹æ•°æ®é¢„å¤„ç†å®Œæˆã€‚\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. åˆ¶ä½œåºåˆ— (éœ€è¦è·å– S_src ç”¨äºå¹³è¡¡)\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- [æ­¥éª¤ 2] åˆ¶ä½œåºåˆ— ---\")\n",
    "    # æ³¨æ„ï¼šcreate_sequences_for_svm éœ€è¦è¿”å› S_src (å«æ˜Ÿç±»å‹)\n",
    "    X_src, y_src, S_src, I_src_raw, X_tgt, y_tgt, S_tgt, TGT_INDICES = create_sequences_for_svm(df_src, df_tgt, config)\n",
    "    print(\"--- [æ•°æ®åˆ†å¸ƒæ£€æŸ¥] ---\")\n",
    "    from collections import Counter\n",
    "    print(f\"ç›®æ ‡åŸŸå«æ˜Ÿåˆ†å¸ƒ: {Counter(S_tgt)}\")\n",
    "    if X_src is None: return\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. åŠ è½½ç‰¹å¾æå–å™¨ (G_f)\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- [æ­¥éª¤ 3] åŠ è½½ G_f æ¨¡å‹å¹¶æå–ç‰¹å¾ ---\")\n",
    "    # ï¼ï¼è¯·ç¡®ä¿è¿™é‡Œçš„å‚æ•°ä¸ models.py ä¸­çš„å®šä¹‰ä¸€è‡´ï¼ï¼\n",
    "    # å¦‚æœ models.py é‡ŒåŠ äº† final_feature_dimï¼Œè¿™é‡Œä¹Ÿè¦ç¡®è®¤é»˜è®¤å€¼æ˜¯å¦åŒ¹é…\n",
    "    G_f = LSTMFeatureExtractor(\n",
    "        input_size=config.LSTM_INPUT_SIZE,\n",
    "        hidden_size=config.LSTM_HIDDEN_SIZE,\n",
    "        num_layers=config.LSTM_NUM_LAYERS,\n",
    "        dropout=config.LSTM_DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "    G_f_path = os.path.join(config.MODEL_SAVE_DIR, \"G_f_final.pth\")\n",
    "    if not os.path.exists(G_f_path):\n",
    "        print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {G_f_path}\")\n",
    "        return\n",
    "        \n",
    "    G_f.load_state_dict(torch.load(G_f_path, map_location=device))\n",
    "    G_f.eval() # å¿…é¡»æ˜¯ eval æ¨¡å¼ï¼\n",
    "\n",
    "    # æå–ç‰¹å¾ (Raw Features)\n",
    "    F_src_raw = get_features(G_f, X_src, config.BATCH_SIZE, device)\n",
    "    if np.isnan(F_src_raw).any():\n",
    "        print(\"ã€ğŸ’¥ çˆ†ç‚¸ã€‘LSTM æå–å‡ºçš„åŸå§‹ç‰¹å¾é‡Œå°±å·²ç»æœ‰ NaN äº†ï¼æ¨¡å‹è®­ç»ƒåºŸäº†ï¼\")\n",
    "    else:\n",
    "        print(\"ã€âœ… æ­£å¸¸ã€‘åŸå§‹ç‰¹å¾é‡Œæ²¡æœ‰ NaNã€‚é—®é¢˜å‡ºåœ¨æ ‡å‡†åŒ–æ­¥éª¤ã€‚\")\n",
    "    F_tgt_raw = get_features(G_f, X_tgt, config.BATCH_SIZE, device)\n",
    "    \n",
    "    print(f\"  - æºåŸŸç‰¹å¾å½¢çŠ¶: {F_src_raw.shape}\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4. ç‰¹å¾æ ‡å‡†åŒ– (Feature Standardization)\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- [æ­¥éª¤ 4] ç‰¹å¾æ ‡å‡†åŒ– ---\")\n",
    "    # å…ˆæ ‡å‡†åŒ–ï¼Œå†å¹³è¡¡ã€‚è¿™æ ·Scaleræ˜¯åŸºäºå…¨é‡æ•°æ®çš„ï¼Œæ›´å‡†ã€‚\n",
    "    F_src_scaled, F_tgt_scaled = standardize_features_global(F_src_raw, F_tgt_raw)\n",
    "    \n",
    "    # æ¸…æ´— (NaN/Inf)\n",
    "    F_src_scaled = clean_features(F_src_scaled)\n",
    "    F_tgt_scaled = clean_features(F_tgt_scaled)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 5. æ•°æ®å¹³è¡¡ (å››è±¡é™å¹³è¡¡) & è®­ç»ƒ SVM\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- [æ­¥éª¤ 5] æ•°æ®å¹³è¡¡ä¸ SVM è®­ç»ƒ ---\")\n",
    "    \n",
    "    # ä½¿ç”¨ Complex Balance (åŸºäº y å’Œ S)\n",
    "    F_bal, y_bal = complex_balance_data(F_src_scaled, y_src, S_src)\n",
    "    \n",
    "    # äºŒæ¬¡é‡‡æ · (å¦‚æœæ•°æ®é‡å¤ªå¤§ï¼ŒSVM è·‘ä¸åŠ¨)\n",
    "    MAX_SVM_SAMPLES = 20000\n",
    "    if len(F_bal) > MAX_SVM_SAMPLES:\n",
    "        print(f\"  - æ•°æ®é‡ {len(F_bal)} > {MAX_SVM_SAMPLES}ï¼Œè¿›è¡ŒäºŒæ¬¡éšæœºé‡‡æ ·...\")\n",
    "        # è¿™é‡Œåªéœ€è¦éšæœºé‡‡æ ·å³å¯ï¼Œå› ä¸º F_bal å·²ç»æ˜¯æ¯”ä¾‹å¹³è¡¡çš„äº†\n",
    "        indices = np.random.choice(len(F_bal), MAX_SVM_SAMPLES, replace=False)\n",
    "        F_bal = F_bal[indices]\n",
    "        y_bal = y_bal[indices]\n",
    "\n",
    "    print(f\"  - æœ€ç»ˆ SVM è®­ç»ƒé›†å¤§å°: {F_bal.shape}\")\n",
    "\n",
    "    # è®­ç»ƒ\n",
    "    print(\"  - å¼€å§‹è®­ç»ƒ SVM (RBF Kernel)...\")\n",
    "    svm_global = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42) \n",
    "    # C=1.0 é€šå¸¸æ¯” 0.5 æ›´é€šç”¨ï¼Œgamma='scale' è‡ªåŠ¨å¤„ç†ç‰¹å¾ç»´åº¦\n",
    "    svm_global.fit(F_bal, y_bal)\n",
    "    print(\"âœ… SVM è®­ç»ƒå®Œæˆã€‚\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 6. é¢„æµ‹ä¸è¯„ä¼°\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- [æ­¥éª¤ 6] ç›®æ ‡åŸŸé¢„æµ‹ ---\")\n",
    "    if len(F_tgt_scaled) > 0:\n",
    "        # åˆ†æ‰¹é¢„æµ‹é˜²æ­¢å†…å­˜æº¢å‡º (è™½ä¸€èˆ¬ä¸éœ€è¦ï¼Œä½†ä¿é™©)\n",
    "        y_pred_tgt = svm_global.predict(F_tgt_scaled)\n",
    "        \n",
    "        # è¯„ä¼° (å¦‚æœæœ‰çœŸå®æ ‡ç­¾)\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ y_tgt æ˜¯æœ‰æ•ˆçš„ï¼Œå¦‚æœå…¨æ˜¯å ä½ç¬¦ 0ï¼Œå‡†ç¡®ç‡æ— æ„ä¹‰\n",
    "        unique_tgt_labels = np.unique(y_tgt)\n",
    "        if len(unique_tgt_labels) > 1: # åªæœ‰å½“ç›®æ ‡åŸŸåŒ…å«è‡³å°‘ä¸¤ç§æ ‡ç­¾æ—¶æ‰è¯„ä¼°\n",
    "            print(\"  - æ­£åœ¨è¯„ä¼°ç›®æ ‡åŸŸå‡†ç¡®ç‡...\")\n",
    "            print(classification_report(y_tgt, y_pred_tgt, zero_division=0))\n",
    "        else:\n",
    "            print(\"  - ç›®æ ‡åŸŸæ— æœ‰æ•ˆæ ‡ç­¾ (æˆ–ä»…æœ‰ä¸€ç§æ ‡ç­¾)ï¼Œè·³è¿‡ Accuracy è¯„ä¼°ã€‚\")\n",
    "    else:\n",
    "        print(\"âŒ è­¦å‘Šï¼šç›®æ ‡åŸŸç‰¹å¾ä¸ºç©ºã€‚\")\n",
    "        return\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 7. ç»“æœä¿å­˜ (å›æº¯)\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- [æ­¥éª¤ 7] ä¿å­˜ç»“æœ ---\")\n",
    "    full_predictions = pd.Series(data=np.nan, index=df_tgt.index, dtype=float)\n",
    "    full_predictions.loc[TGT_INDICES] = y_pred_tgt\n",
    "    \n",
    "    df_tgt['predicted_multipath'] = full_predictions\n",
    "    \n",
    "    OUTPUT_DIR = os.path.join(config.MODEL_SAVE_DIR, \"results_single_svm\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"tgt_data_with_single_svm_predictions.csv\")\n",
    "    \n",
    "    df_tgt.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"âœ… ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_single_svm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
