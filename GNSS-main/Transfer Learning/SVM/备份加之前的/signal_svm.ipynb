{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0288b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Config] 正在使用的设备: cpu ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# 确保导入了您所有的配置、模型定义和工具函数\n",
    "import config \n",
    "from models import LSTMFeatureExtractor \n",
    "from utility_uad_svm import load_data, create_sequences_for_svm\n",
    "from utility_uad_svm import get_features, standardize_features # 注意：不再使用 complex_balance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2e4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_balance_data(X_feat, y_labels, random_state=42):\n",
    "    \"\"\"\n",
    "    仅基于标签 (y_labels) 对特征进行欠采样，实现 LOS/NLOS 平衡。\n",
    "    \"\"\"\n",
    "    print(f\"  - 原始类别分布: {Counter(y_labels)}\")\n",
    "    \n",
    "    # 找到 LOS (0) 和 NLOS (1) 的索引\n",
    "    idx_los = np.where(y_labels == 0)[0]\n",
    "    idx_nlos = np.where(y_labels == 1)[0]\n",
    "    \n",
    "    # 确定最小组大小\n",
    "    min_size = min(len(idx_los), len(idx_nlos))\n",
    "    \n",
    "    if min_size == 0:\n",
    "        print(\"警告: 某一类样本数为零，无法平衡。返回空数组。\")\n",
    "        return np.empty((0, X_feat.shape[1])), np.empty((0,))\n",
    "\n",
    "    # 对索引进行欠采样\n",
    "    np.random.seed(random_state)\n",
    "    idx_los_bal = np.random.choice(idx_los, min_size, replace=False)\n",
    "    idx_nlos_bal = np.random.choice(idx_nlos, min_size, replace=False)\n",
    "\n",
    "    # 合并索引并打乱\n",
    "    combined_idx = np.concatenate([idx_los_bal, idx_nlos_bal])\n",
    "    np.random.shuffle(combined_idx)\n",
    "    \n",
    "    # 重新映射到特征和标签\n",
    "    X_resampled = X_feat[combined_idx]\n",
    "    y_resampled = y_labels[combined_idx]\n",
    "\n",
    "    print(f\"✅ 特征空间简单平衡完成。每类 {min_size} 条，总样本数: {len(X_resampled)}\")\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56edfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(F_feat):\n",
    "    \"\"\"清除特征中的 NaN 和 Inf 值。\"\"\"\n",
    "    nan_count = np.sum(np.isnan(F_feat))\n",
    "    inf_count = np.sum(np.isinf(F_feat))\n",
    "    \n",
    "    if nan_count > 0 or inf_count > 0:\n",
    "        print(f\"【⚠️ 再次清理】发现 {nan_count} 个 NaN 和 {inf_count} 个 Inf。正在执行 nan_to_num...\")\n",
    "        # nan_to_num 会将 NaN 替换为 0，将 Inf 替换为数值极限\n",
    "        F_feat = np.nan_to_num(F_feat, nan=0.0, posinf=1e8, neginf=-1e8) \n",
    "        print(\"清理完成。\")\n",
    "    return F_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4c0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(config.DEVICE)\n",
    "SRC_IDS = config.SRC_IDS\n",
    "TGT_ID = config.TGT_ID\n",
    "CSV_PATHS = config.CSV_PATHS\n",
    "MAX_SVM_SAMPLES = 20000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf67c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SVM Main] 正在使用设备: cpu ---\n",
      "\n",
      "--- [步骤 1] 正在加载原始数据和执行原始数据标准化... ---\n",
      "[load_data] 源域数据形状: (710063, 13), 目标域数据形状: (12614, 14)\n",
      "✅ 原始数据标准化完成。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "print(f\"--- [SVM Main] 正在使用设备: {device} ---\")\n",
    "    \n",
    "    # --- 1. 加载原始数据并标准化 ---\n",
    "print(\"\\n--- [步骤 1] 正在加载原始数据和执行原始数据标准化... ---\")\n",
    "df_src, df_tgt = load_data(SRC_IDS, TGT_ID, CSV_PATHS)\n",
    "    \n",
    "    # ！！ 关键修正：原始数据标准化 ！！\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_src_features = df_src[config.FEATURES].copy()\n",
    "df_tgt_features = df_tgt[config.FEATURES].copy()\n",
    "\n",
    "df_src[config.FEATURES] = scaler.fit_transform(df_src_features)\n",
    "df_tgt[config.FEATURES] = scaler.transform(df_tgt_features)\n",
    "print(\"✅ 原始数据标准化完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "607ecb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SVM Main] 正在使用设备: cpu ---\n",
      "\n",
      "--- [步骤 1] 正在加载原始数据和执行原始数据标准化... ---\n",
      "[load_data] 源域数据形状: (710063, 13), 目标域数据形状: (12614, 14)\n",
      "✅ 原始数据标准化完成。\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- [SVM Main] 正在使用设备: {device} ---\")\n",
    "    \n",
    "    # --- 1. 加载原始数据并标准化 ---\n",
    "print(\"\\n--- [步骤 1] 正在加载原始数据和执行原始数据标准化... ---\")\n",
    "df_src, df_tgt = load_data(SRC_IDS, TGT_ID, CSV_PATHS)\n",
    "    \n",
    "# ！！ 关键修正：原始数据标准化 ！！\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_src_features = df_src[config.FEATURES].copy()\n",
    "df_tgt_features = df_tgt[config.FEATURES].copy()\n",
    "\n",
    "df_src[config.FEATURES] = scaler.fit_transform(df_src_features)\n",
    "df_tgt[config.FEATURES] = scaler.transform(df_tgt_features)\n",
    "print(\"✅ 原始数据标准化完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0762cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [序列制作] 正在创建 SVM 所需的序列... ---\n",
      "  - 正在处理源域 (仿真) 数据...\n",
      "  - X 形状: (709919, 10, 5), y 形状: (709919,), S 形状: (709919,)\n",
      "\n",
      "  - 正在处理目标域 (真实) 数据 (无标签处理)...\n",
      "  - X 形状: (12443, 10, 5), y 形状: (0,), S 形状: (12443,)\n",
      "--- 序列制作完成 ---\n",
      "\n",
      "--- [步骤 3] 正在加载 G_f 模型并提取特征... ---\n",
      "  - 正在执行特征提取...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - 提取特征中: 100%|██████████| 11093/11093 [00:52<00:00, 213.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 特征提取完毕。输出形状: (709919, 128)\n",
      "  - 正在执行特征提取...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - 提取特征中: 100%|██████████| 195/195 [00:01<00:00, 170.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 特征提取完毕。输出形状: (12443, 128)\n",
      "\n",
      "--- [标准化] 正在对特征进行标准化... ---\n",
      "  - 特征标准化完成 (基于源域统计量)。\n",
      "【⚠️ 再次清理】发现 1240320 个 NaN 和 0 个 Inf。正在执行 nan_to_num...\n",
      "清理完成。\n",
      "\n",
      "--- [步骤 5] 整体平衡源域数据 (GPS + PL) 并训练单一 SVM ---\n",
      "  - 原始类别分布: Counter({np.int64(1): 376554, np.int64(0): 333365})\n",
      "✅ 特征空间简单平衡完成。每类 333365 条，总样本数: 666730\n",
      " - 训练集规模已从 709919 减少到 20000。\n",
      "\n",
      "--- 训练单一全局 SVM 模型 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 全局 SVM 训练完成。训练样本数: 20000\n",
      "\n",
      "--- [步骤 6] 目标域预测 (单一模型)... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [步骤 7] 评估和报告 ---\n",
      "\n",
      "✅ 预测完成。目标域无标签或标签为占位符，跳过评估。\n",
      "\n",
      "--- [步骤 8] 预测结果回溯合并与保存 ---\n",
      "警告：缺少 df_tgt、TGT_INDICES 或 y_pred_tgt 变量，跳过结果文件保存。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SVM 单专家主函数\n",
    "# =============================================================================\n",
    "def main_single_svm():\n",
    "    # ... (与之前相同的全局配置加载) ...\n",
    "    # --- 2. 创建序列 ---\n",
    "    X_src, y_src, S_src, I_src_raw, X_tgt, y_tgt, S_tgt, TGT_INDICES = create_sequences_for_svm(df_src, df_tgt, config)\n",
    "    if X_src is None or X_tgt is None: return\n",
    "\n",
    "    # --- 3. 特征提取 (加载 G_f 模型) ---\n",
    "    print(\"\\n--- [步骤 3] 正在加载 G_f 模型并提取特征... ---\")\n",
    "    G_f = LSTMFeatureExtractor(\n",
    "        input_size=config.LSTM_INPUT_SIZE, hidden_size=config.LSTM_HIDDEN_SIZE,\n",
    "        num_layers=config.LSTM_NUM_LAYERS, dropout=config.LSTM_DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "    G_f_path = os.path.join(config.MODEL_SAVE_DIR, \"G_f_final.pth\")\n",
    "    try:\n",
    "        G_f.load_state_dict(torch.load(G_f_path, map_location=device))\n",
    "        G_f.eval()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 错误：未找到 G_f 权重文件。请确保 DANN 训练已完成。\")\n",
    "        return\n",
    "\n",
    "    F_src_raw = get_features(G_f, X_src, config.BATCH_SIZE, device)\n",
    "    F_tgt_raw = get_features(G_f, X_tgt, config.BATCH_SIZE, device)\n",
    "\n",
    "    # --- 4. 特征标准化 (对 256 维特征进行第二次标准化) ---\n",
    "    F_src_scaled, F_tgt_scaled = standardize_features(F_src_raw, F_tgt_raw)\n",
    "\n",
    "    # 强制清洗源域和目标域的缩放后特征\n",
    "    F_src_scaled = clean_features(F_src_scaled)\n",
    "    F_tgt_scaled = clean_features(F_tgt_scaled)    \n",
    "        # =============================================================================\n",
    "        # 5. 【核心修改】整体平衡和训练单一 SVM\n",
    "        # =============================================================================\n",
    "    print(\"\\n--- [步骤 5] 整体平衡源域数据 (GPS + PL) 并训练单一 SVM ---\")\n",
    "\n",
    "    # 5a. 整体平衡源域特征\n",
    "    F_bal, y_bal = simple_balance_data(F_src_scaled, y_src, random_state=42)\n",
    "\n",
    "    # 5b. 二次欠采样 (控制规模)\n",
    "    if len(F_bal) > MAX_SVM_SAMPLES:\n",
    "        F_bal, _, y_bal, _ = train_test_split(F_bal, y_bal, train_size=MAX_SVM_SAMPLES, random_state=42, stratify=y_bal)\n",
    "        print(f\" - 训练集规模已从 {len(F_src_scaled)} 减少到 {len(F_bal)}。\")\n",
    "    else:\n",
    "         print(f\" - 训练集规模: {len(F_bal)}。无需二次采样。\")\n",
    "    \n",
    "    # 5c. 训练单一 SVM 模型\n",
    "    print(\"\\n--- 训练单一全局 SVM 模型 ---\")\n",
    "    svm_global = SVC(kernel='rbf', C=0.5, random_state=42)\n",
    "    if len(F_bal) > 0:\n",
    "        with tqdm(total=1, desc=\"训练全局 SVM 模型\", ncols=80, leave=False):\n",
    "            svm_global.fit(F_bal, y_bal)\n",
    "        print(f\"✅ 全局 SVM 训练完成。训练样本数: {len(F_bal)}\")\n",
    "    else:\n",
    "        print(\" - 警告：SVM 样本不足，跳过训练。\")\n",
    "\n",
    "    # --- 6. 目标域预测 (单一模型预测所有特征) ---\n",
    "    print(\"\\n--- [步骤 6] 目标域预测 (单一模型)... ---\")\n",
    "    \n",
    "    if 'svm_global' in locals() and len(F_tgt_scaled) > 0:\n",
    "        with tqdm(total=len(F_tgt_scaled), desc=\"  - 全局模型预测中\", leave=False) as pbar:\n",
    "            y_pred_tgt = svm_global.predict(F_tgt_scaled)\n",
    "            pbar.update(len(F_tgt_scaled))\n",
    "    else:\n",
    "        print(\"警告：无法预测，请检查模型是否训练成功或目标域数据量。\")\n",
    "        return\n",
    "\n",
    "    # --- 7. 评估和保存结果 (与之前的逻辑相同) ---\n",
    "    print(\"\\n--- [步骤 7] 评估和报告 ---\")\n",
    "    # ... (评估代码与之前相同) ...\n",
    "    if len(y_tgt) > 0 and not np.all(y_tgt == 0):\n",
    "        accuracy = accuracy_score(y_tgt, y_pred_tgt)\n",
    "        report = classification_report(y_tgt, y_pred_tgt, output_dict=False, zero_division=0)\n",
    "        \n",
    "        print(\"\\n=======================================================\")\n",
    "        print(\"✅ DANN-LSTM 单一全局 SVM 性能报告：\")\n",
    "        print(f\"目标域总体 SVM 准确率 (Accuracy): {accuracy:.4f}\")\n",
    "        print(\"目标域 SVM 评估报告:\\n\", report)\n",
    "        print(\"=======================================================\")\n",
    "    else:\n",
    "        print(\"\\n✅ 预测完成。目标域无标签或标签为占位符，跳过评估。\")\n",
    "\n",
    "    # ... (保存预测结果到 CSV 的代码与之前相同) ...\n",
    "    # =============================================================================\n",
    "# 8. 预测结果回溯合并与保存\n",
    "# =============================================================================\n",
    "    print(\"\\n--- [步骤 8] 预测结果回溯合并与保存 ---\")\n",
    "\n",
    "    # 检查所需变量是否存在，确保流程安全\n",
    "    if 'df_tgt' in locals() and 'TGT_INDICES' in locals() and 'y_pred_tgt' in locals():\n",
    "        \n",
    "        # 1. 创建一个与原始 df_tgt 长度相同的空预测列\n",
    "        # 使用 NaN 作为占位符，因为不是所有原始行都能形成一个序列的尾部。\n",
    "        full_predictions = pd.Series(\n",
    "            data=np.nan, \n",
    "            index=df_tgt.index, \n",
    "            dtype=float \n",
    "        )\n",
    "\n",
    "        # 2. ！！关键的回溯操作：使用 TGT_INDICES 映射预测值 ！！\n",
    "        # TGT_INDICES 包含了每个预测值对应的原始 df_tgt 的索引。\n",
    "        full_predictions.loc[TGT_INDICES] = y_pred_tgt\n",
    "\n",
    "        # 3. 将新的预测列附加到原始 DataFrame\n",
    "        df_tgt['predicted_multipath'] = full_predictions\n",
    "\n",
    "        # 4. 定义输出并保存文件\n",
    "        OUTPUT_DIR = os.path.join(config.MODEL_SAVE_DIR, \"results_single_svm\")\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"tgt_data_with_single_svm_predictions.csv\")\n",
    "\n",
    "        df_tgt.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "        print(f\"✅ 最终结果文件已成功保存到: {OUTPUT_PATH}\")\n",
    "        print(\"文件已包含所有原始列，并在末尾附加了 'predicted_multipath' 列。\")\n",
    "    else:\n",
    "        print(\"警告：缺少 df_tgt、TGT_INDICES 或 y_pred_tgt 变量，跳过结果文件保存。\")\n",
    "# 运行主函数\n",
    "if __name__ == '__main__':\n",
    "    main_single_svm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
