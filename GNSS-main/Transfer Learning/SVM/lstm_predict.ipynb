{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ce6490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [LSTM Inference] 正在使用设备: cpu ---\n",
      "\n",
      "--- [步骤 1] 加载训练好的模型 ---\n",
      "✅ 模型加载成功。\n",
      "\n",
      "--- [步骤 2] 准备目标域数据 ---\n",
      "[load_data] 正在加载目标域数据: 0\n",
      "[load_data] 目标域加载完成。形状: (12614, 14)\n",
      "[load_data] 注：源域数据将在 'create_all_sequences' 中逐个加载以保证时序独立性。\n",
      " -> 应用 DANN 训练时的标准化参数...\n",
      " -> 制作时间序列...\n",
      "  - X 形状: (12538, 5, 5), y 形状: (0,), S 形状: (12538,)\n",
      " -> 序列制作完成。样本数: 12538\n",
      "\n",
      "--- [步骤 3] 执行 LSTM 推理 ---\n",
      "\n",
      "--- [步骤 4] 统计与保存 ---\n",
      " -> 预测为坏信号 (1) 的比例: 17.54%\n",
      " -> 预测为好信号 (0) 的比例: 82.46%\n",
      "✅ 结果合理：符合 Aachen 环境约 80% 好信号的先验知识。\n",
      "✅ 预测结果已保存至: c:\\Users\\yangj\\Desktop\\GNSS-main\\GNSS-main\\Transfer Learning\\SVM\\checkpoints\\results_lstm\\tgt_data_with_lstm_predictions.csv\n",
      "   (包含列 'predicted_multipath' 和 'prob_score')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 引入你的配置和工具\n",
    "import config\n",
    "from models import LSTMFeatureExtractor, LabelClassifier\n",
    "from utility_uad_svm import load_data, make_sequences_for_svm, lstm_standardization_train_pre_svm, SeqDataset\n",
    "\n",
    "def main_lstm_inference():\n",
    "    # 1. 设备设置\n",
    "    device = torch.device(config.DEVICE)\n",
    "    print(f\"--- [LSTM Inference] 正在使用设备: {device} ---\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. 加载模型 (G_f 和 G_y)\n",
    "    # ==========================================\n",
    "    print(\"\\n--- [步骤 1] 加载训练好的模型 ---\")\n",
    "    \n",
    "    # 实例化特征提取器 G_f\n",
    "    G_f = LSTMFeatureExtractor(\n",
    "        input_size=config.LSTM_INPUT_SIZE,\n",
    "        hidden_size=config.LSTM_HIDDEN_SIZE,\n",
    "        num_layers=config.LSTM_NUM_LAYERS,\n",
    "        dropout=config.LSTM_DROPOUT\n",
    "    ).to(device)\n",
    "    \n",
    "    # 实例化分类器 G_y\n",
    "    G_y = LabelClassifier(\n",
    "        input_size=config.LSTM_HIDDEN_SIZE,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        hidden_dim=config.CLASSIFIER_HIDDEN_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    # 加载权重\n",
    "    gf_path = os.path.join(config.MODEL_SAVE_DIR, \"G_f_final.pth\")\n",
    "    gy_path = os.path.join(config.MODEL_SAVE_DIR, \"G_y_final.pth\")\n",
    "    \n",
    "    if not os.path.exists(gf_path) or not os.path.exists(gy_path):\n",
    "        print(\"❌ 错误：找不到模型权重文件，请先运行 DANN 训练！\")\n",
    "        return\n",
    "\n",
    "    G_f.load_state_dict(torch.load(gf_path, map_location=device))\n",
    "    G_y.load_state_dict(torch.load(gy_path, map_location=device))\n",
    "    \n",
    "    # 【关键】设置为评估模式 (关闭 Dropout/BatchNorm 更新)\n",
    "    G_f.eval()\n",
    "    G_y.eval()\n",
    "    print(\"✅ 模型加载成功。\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. 数据准备 (加载 -> 标准化 -> 序列化)\n",
    "    # ==========================================\n",
    "    print(\"\\n--- [步骤 2] 准备目标域数据 ---\")\n",
    "    \n",
    "    # 3.1 加载原始数据\n",
    "    # 这里我们只需要目标域数据，所以 src_ids 随便传，只要 tgt_id 对就行\n",
    "    _, df_tgt = load_data(config.SRC_IDS, config.TGT_ID, config.CSV_PATHS)\n",
    "    \n",
    "    if df_tgt is None: return\n",
    "    \n",
    "    # 3.2 备份原始数据 (用于最后保存结果)\n",
    "    df_tgt_raw = df_tgt.copy()\n",
    "    \n",
    "    # 清洗\n",
    "    if df_tgt.isnull().values.any(): df_tgt = df_tgt.fillna(0)\n",
    "    df_tgt = df_tgt.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # 3.3 标准化 (必须加载 DANN 训练时的 Scaler)\n",
    "    print(\" -> 应用 DANN 训练时的标准化参数...\")\n",
    "    df_tgt = lstm_standardization_train_pre_svm(\n",
    "        df_tgt, \n",
    "        config.FEATURES, \n",
    "        config.MODEL_SAVE_DIR\n",
    "    )\n",
    "\n",
    "    # 3.4 制作序列\n",
    "    # 我们复用 make_sequences_for_svm，因为它能返回索引 I，方便我们把预测结果填回原表\n",
    "    print(\" -> 制作时间序列...\")\n",
    "    X_tgt, _, _, TGT_INDICES = make_sequences_for_svm(\n",
    "        df=df_tgt,\n",
    "        features=config.FEATURES,\n",
    "        target=None, # 无标签预测\n",
    "        seq_len=config.SEQ_LEN,\n",
    "        step=config.STEP\n",
    "    )\n",
    "    \n",
    "    if len(X_tgt) == 0:\n",
    "        print(\"❌ 错误：没有生成有效序列。\")\n",
    "        return\n",
    "\n",
    "    print(f\" -> 序列制作完成。样本数: {len(X_tgt)}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. 执行预测\n",
    "    # ==========================================\n",
    "    print(\"\\n--- [步骤 3] 执行 LSTM 推理 ---\")\n",
    "    \n",
    "    # 创建 DataLoader (为了显存安全，批量预测)\n",
    "    # y 传 None 即可\n",
    "    ds_tgt = SeqDataset(X_tgt, y=None)\n",
    "    dl_tgt = DataLoader(ds_tgt, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_probs_good = [] # 存储 \"是好信号\" 的概率\n",
    "    \n",
    "    with torch.no_grad(): # 这一步不需要计算梯度，省内存\n",
    "        for X_batch, _ in dl_tgt:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # 1. 提取特征\n",
    "            features = G_f(X_batch)\n",
    "            \n",
    "            # 2. 分类 (输出 Logits)\n",
    "            logits = G_y(features)\n",
    "            \n",
    "            # 3. 计算概率 (Softmax)\n",
    "            # dim=1 表示在类别维度计算。输出形状 [batch, 2]\n",
    "            # [:, 0] 是好信号概率，[:, 1] 是坏信号概率 (假设 0=Good, 1=Bad)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # 4. 获取硬分类结果 (最大概率对应的索引)\n",
    "            preds_batch = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # 收集结果\n",
    "            all_preds.append(preds_batch.cpu().numpy())\n",
    "            # 收集 \"好信号(Class 0)\" 的概率，用于后续 Soft-WLS 权重\n",
    "            all_probs_good.append(probs[:, 0].cpu().numpy())\n",
    "\n",
    "    # 合并批次结果\n",
    "    final_preds = np.concatenate(all_preds)\n",
    "    final_probs_good = np.concatenate(all_probs_good)\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. 结果统计与保存\n",
    "    # ==========================================\n",
    "    print(\"\\n--- [步骤 4] 统计与保存 ---\")\n",
    "    \n",
    "    # 统计比例\n",
    "    ratio_bad = np.sum(final_preds == 1) / len(final_preds)\n",
    "    print(f\" -> 预测为坏信号 (1) 的比例: {ratio_bad:.2%}\")\n",
    "    print(f\" -> 预测为好信号 (0) 的比例: {1-ratio_bad:.2%}\")\n",
    "    \n",
    "    # 物理合理性检查\n",
    "    if 0.10 <= ratio_bad <= 0.40:\n",
    "        print(\"✅ 结果合理：符合 Aachen 环境约 80% 好信号的先验知识。\")\n",
    "    else:\n",
    "        print(\"⚠️ 结果警告：比例可能偏离预期，请检查标准化或模型收敛情况。\")\n",
    "\n",
    "    # 保存回原始 DataFrame\n",
    "    # 初始化全为 NaN\n",
    "    df_tgt_raw['predicted_multipath'] = np.nan\n",
    "    df_tgt_raw['prob_score'] = np.nan # 新增一列：LSTM认为它是好信号的概率\n",
    "    \n",
    "    # 根据索引填入\n",
    "    # 注意：序列化会丢掉前 seq_len-1 个点，所以必须用 TGT_INDICES 对齐\n",
    "    df_tgt_raw.loc[TGT_INDICES, 'predicted_multipath'] = final_preds\n",
    "    df_tgt_raw.loc[TGT_INDICES, 'prob_score'] = final_probs_good\n",
    "    \n",
    "    # 剔除开头没法预测的 NaN 行 (可选，看你解算器是否健壮)\n",
    "    # df_result = df_tgt_raw.dropna(subset=['predicted_multipath']) \n",
    "    \n",
    "    # 路径设置\n",
    "    save_dir = os.path.join(config.MODEL_SAVE_DIR, \"results_lstm\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, \"tgt_data_with_lstm_predictions.csv\")\n",
    "    \n",
    "    df_tgt_raw.to_csv(save_path, index=False)\n",
    "    print(f\"✅ 预测结果已保存至: {save_path}\")\n",
    "    print(f\"   (包含列 'predicted_multipath' 和 'prob_score')\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_lstm_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
